{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grad-CAM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh1cEFJAi5_S",
        "outputId": "d5cbc2b7-ac4a-486d-d53c-68483c9e7840"
      },
      "source": [
        "# this is the exact copy of the keras example with no changes.\n",
        "\n",
        "\"\"\"\n",
        "Title: Simple MNIST convnet\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2015/06/19\n",
        "Last modified: 2020/04/21\n",
        "Description: A simple convnet that achieves ~99% test accuracy on MNIST.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "\"\"\"\n",
        "## Evaluate the trained model\n",
        "\"\"\"\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n",
        "model.save_weights('mnist.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "422/422 [==============================] - 6s 9ms/step - loss: 0.3666 - accuracy: 0.8887 - val_loss: 0.0851 - val_accuracy: 0.9773\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.1153 - accuracy: 0.9652 - val_loss: 0.0641 - val_accuracy: 0.9830\n",
            "Test loss: 0.06627397984266281\n",
            "Test accuracy: 0.9789999723434448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6Nmu0H2-hRR"
      },
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model, classIdx, layerName=None):\n",
        "        # store the model, the class index used to measure the class\n",
        "        # activation map, and the layer to be used when visualizing\n",
        "        # the class activation map\n",
        "        self.model = model\n",
        "        self.classIdx = classIdx\n",
        "        self.layerName = layerName\n",
        "        # if the layer name is None, attempt to automatically find\n",
        "        # the target output layer\n",
        "        if self.layerName is None:\n",
        "            self.layerName = self.find_target_layer()\n",
        " \n",
        "    def find_target_layer(self):\n",
        "        # attempt to find the final convolutional layer in the network\n",
        "        # by looping over the layers of the network in reverse order\n",
        "        for layer in reversed(self.model.layers):\n",
        "            # check to see if the layer has a 4D output\n",
        "            if len(layer.output_shape) == 4:\n",
        "                return layer.name\n",
        "        # otherwise, we could not find a 4D layer so the GradCAM\n",
        "        # algorithm cannot be applied\n",
        "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
        " \n",
        " \n",
        "    def compute_heatmap(self, image, eps=1e-8):\n",
        "        # construct our gradient model by supplying (1) the inputs\n",
        "        # to our pre-trained model, (2) the output of the (presumably)\n",
        "        # final 4D layer in the network, and (3) the output of the\n",
        "        # softmax activations from the model\n",
        "        gradModel = Model(\n",
        "            inputs=[self.model.inputs],\n",
        "            outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n",
        " \n",
        "        # record operations for automatic differentiation\n",
        "        with tf.GradientTape() as tape:\n",
        "            # cast the image tensor to a float-32 data type, pass the\n",
        "            # image through the gradient model, and grab the loss\n",
        "            # associated with the specific class index\n",
        "            inputs = tf.cast(image, tf.float32)\n",
        "            (convOutputs, predictions) = gradModel(inputs)\n",
        "            \n",
        "            loss = predictions[:, tf.argmax(predictions[0])]\n",
        "    \n",
        "        # use automatic differentiation to compute the gradients\n",
        "        grads = tape.gradient(loss, convOutputs)\n",
        " \n",
        "        # compute the guided gradients\n",
        "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
        "        castGrads = tf.cast(grads > 0, \"float32\")\n",
        "        guidedGrads = castConvOutputs * castGrads * grads\n",
        "        # the convolution and guided gradients have a batch dimension\n",
        "        # (which we don't need) so let's grab the volume itself and\n",
        "        # discard the batch\n",
        "        convOutputs = convOutputs[0]\n",
        "        guidedGrads = guidedGrads[0]\n",
        " \n",
        "        # compute the average of the gradient values, and using them\n",
        "        # as weights, compute the ponderation of the filters with\n",
        "        # respect to the weights\n",
        "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
        "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
        " \n",
        "        # grab the spatial dimensions of the input image and resize\n",
        "        # the output class activation map to match the input image\n",
        "        # dimensions\n",
        "        (w, h) = (image.shape[2], image.shape[1])\n",
        "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
        "        # normalize the heatmap such that all values lie in the range\n",
        "        # [0, 1], scale the resulting values to the range [0, 255],\n",
        "        # and then convert to an unsigned 8-bit integer\n",
        "        numer = heatmap - np.min(heatmap)\n",
        "        denom = (heatmap.max() - heatmap.min()) + eps\n",
        "        heatmap = numer / denom\n",
        "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
        "        # return the resulting heatmap to the calling function\n",
        "        return heatmap\n",
        " \n",
        "    def overlay_heatmap(self, heatmap, image, alpha=0.3,\n",
        "                        colormap=cv2.COLORMAP_JET):\n",
        "        # apply the supplied color map to the heatmap and then\n",
        "        # overlay the heatmap on the input image\n",
        "        \n",
        "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
        "        heatmap = cv2.bitwise_not(heatmap)\n",
        "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
        "        # return a 2-tuple of the color mapped heatmap and the output,\n",
        "        # overlaid image\n",
        "        return (heatmap, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voryhazs-48e",
        "outputId": "8b68a09b-ebcf-4df0-dcdf-c0a07b722e4e"
      },
      "source": [
        "# we use the model prediction on one sample to create the GRAD-CAM\n",
        "\n",
        "pred = model.predict(x_train[0].reshape(1, 28, 28, 1))\n",
        "i = np.argmax(pred)\n",
        "i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvS7ydB8-oEW"
      },
      "source": [
        "# initiate the class\n",
        "\n",
        "grad_cam = GradCAM(model, i, 'conv2d_1')\n",
        "heatmap = grad_cam.compute_heatmap(x_train[0].reshape(1, 28, 28, 1))\n",
        "heatmap = cv2.resize(heatmap, (28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo76JNcJAbn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f912b093-b384-4c0e-82fe-ff823480320e"
      },
      "source": [
        "sample = x_train[0].reshape(28, 28)\n",
        "print(sample.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCmPYAnHnfqy",
        "outputId": "fee93bba-cdf2-471c-a8a7-c1d97dddf518"
      },
      "source": [
        "sample.shape == heatmap.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxQnthztoV-S"
      },
      "source": [
        "# reveret the sample to a normal 8bit image.\n",
        "\n",
        "sample = sample * 255\n",
        "sample = sample.astype('uint8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lsMhUm-omof",
        "outputId": "0b37342e-0fd7-4cc6-d162-1724add27e44"
      },
      "source": [
        "sample.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9KUyZvtooWw",
        "outputId": "e5bebc28-4e95-4ad2-f63f-5357c683a918"
      },
      "source": [
        "heatmap.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtV0EIbHo1JP",
        "outputId": "3be2d8be-0932-4ac1-e8f2-56ff9306aace"
      },
      "source": [
        "# a color map is added to the heatmap changing its channels to 3\n",
        "# so the sample image must also have 3 channels for the final weighted summation\n",
        "# of the heatmap and the sample\n",
        "\n",
        "sample = cv2.resize(sample, (28, 28))\n",
        "sample = cv2.cvtColor(sample, cv2.COLOR_GRAY2BGR)\n",
        "heatmap = cv2.resize(heatmap, (28, 28))\n",
        "print(sample.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af9KaU1n_b7G"
      },
      "source": [
        "# get the final output\n",
        "\n",
        "(heatmap_out, output) = grad_cam.overlay_heatmap(heatmap, sample, alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "MF0AgZTjqBBw",
        "outputId": "5677a1a9-98ff-4ae3-cfa6-db1e5f80ea4e"
      },
      "source": [
        "plt.imshow(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f74703b7950>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}